"""
üî• MULTIMODAL DEEPFAKE DETECTION - FIXED VERSION

Fixes Applied:
1. ‚úÖ Proper model output verification
2. ‚úÖ Correct label convention handling
3. ‚úÖ Balanced fusion weights
4. ‚úÖ Proper evaluation with ground truth
5. ‚úÖ Research questions answered
6. ‚úÖ Statistical analysis

Author: Fixed by AI Assistant
Date: 2025
"""

import os
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import librosa
import timm
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import torchvision.transforms as transforms
import warnings
import json
from collections import defaultdict

warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"[OK] Device: {device}")

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================
class AudioXceptionClassifier(nn.Module):
    """Audio model - Complex classifier (2048 ‚Üí 512 ‚Üí 256 ‚Üí 2)"""
    def __init__(self, num_classes=2, dropout=0.5):
        super().__init__()
        self.base = timm.create_model('xception', pretrained=False, num_classes=0)
        self.classifier = nn.Sequential(
            nn.Linear(2048, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        features = self.base(x)
        logits = self.classifier(features)
        return logits


class VideoXceptionModel(nn.Module):
    """Video model - Simple classifier (2048 ‚Üí 2) - SAME AS TRAINING!"""
    def __init__(self, num_classes=2):
        super().__init__()
        # Use timm xception with simple fc head (same as training)
        self.model = timm.create_model('xception', pretrained=False, num_classes=num_classes)
    
    def forward(self, x):
        return self.model(x)


# ============================================================================
# ADAPTIVE FUSION MODULE (FIXED)
# ============================================================================
class AdaptiveFusionModule:
    """
    Implements confidence-aware adaptive fusion
    Based on research proposal equations
    """

    @staticmethod
    def compute_reliability(logits):
        """
        Compute reliability score for a modality
        Formula: ri = max_c(pi,c)
        """
        probs = F.softmax(logits, dim=1)
        reliability = torch.max(probs, dim=1)[0]
        return reliability

    @staticmethod
    def compute_adaptive_weights(r_video, r_audio, tau=1.0):
        """
        Compute adaptive fusion weights based on reliability
        Formula: Œ±i = exp(œÑ * ri) / [exp(œÑ * rv) + exp(œÑ * ra)]
        """
        exp_video = torch.exp(tau * r_video)
        exp_audio = torch.exp(tau * r_audio)

        alpha_video = exp_video / (exp_video + exp_audio)
        alpha_audio = exp_audio / (exp_video + exp_audio)

        return alpha_video, alpha_audio

    @staticmethod
    def fused_prediction(video_logits, audio_logits, alpha_v, alpha_a):
        """
        Combine modalities with adaptive weights
        Formula: Zfused = Œ±v * Zv + Œ±a * Za
        """
        batch_size = video_logits.shape[0]
        device = video_logits.device
        
        # Convert to tensor if needed
        if not isinstance(alpha_v, torch.Tensor):
            alpha_v = torch.tensor(alpha_v, device=device, dtype=torch.float32)
        if not isinstance(alpha_a, torch.Tensor):
            alpha_a = torch.tensor(alpha_a, device=device, dtype=torch.float32)
        
        # Ensure same device
        alpha_v = alpha_v.to(device)
        alpha_a = alpha_a.to(device)
        
        # Get scalar value if tensor
        if alpha_v.numel() == 1:
            alpha_v_val = alpha_v.item() if alpha_v.dim() == 0 else alpha_v[0].item()
            alpha_v = torch.full((batch_size, 1), alpha_v_val, device=device, dtype=torch.float32)
        else:
            # 1D tensor, ensure correct size
            if alpha_v.shape[0] != batch_size:
                alpha_v_val = alpha_v[0].item() if alpha_v.numel() > 0 else 0.5
                alpha_v = torch.full((batch_size, 1), alpha_v_val, device=device, dtype=torch.float32)
            else:
                alpha_v = alpha_v.view(batch_size, 1)
        
        if alpha_a.numel() == 1:
            alpha_a_val = alpha_a.item() if alpha_a.dim() == 0 else alpha_a[0].item()
            alpha_a = torch.full((batch_size, 1), alpha_a_val, device=device, dtype=torch.float32)
        else:
            # 1D tensor, ensure correct size
            if alpha_a.shape[0] != batch_size:
                alpha_a_val = alpha_a[0].item() if alpha_a.numel() > 0 else 0.5
                alpha_a = torch.full((batch_size, 1), alpha_a_val, device=device, dtype=torch.float32)
            else:
                alpha_a = alpha_a.view(batch_size, 1)
        
        Z_fused = alpha_v * video_logits + alpha_a * audio_logits
        P_fused = F.softmax(Z_fused, dim=1)

        return P_fused


# ============================================================================
# MODEL LOADING
# ============================================================================
def load_checkpoint_auto(model, ckpt_path, is_video_model=False):
    """Auto-fix all loading issues"""
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
    state = ckpt["model_state_dict"] if isinstance(ckpt, dict) and "model_state_dict" in ckpt else ckpt
    
    # For VideoXceptionModel, weights are saved directly from timm model
    # Need to add "model." prefix to all keys
    if is_video_model:
        new_state = {}
        for k, v in state.items():
            # Check if already has "model." prefix
            if not k.startswith("model."):
                new_key = "model." + k
            else:
                new_key = k
            new_state[new_key] = v
        model.load_state_dict(new_state, strict=False)
    else:
        # Audio model: remap backbone ‚Üí base
        new_state = {}
        for k, v in state.items():
            new_key = k.replace("backbone.", "base.") if k.startswith("backbone.") else k
            new_state[new_key] = v
        model.load_state_dict(new_state, strict=False)
    
    return ckpt


def load_models(audio_model_path, video_model_path):
    """Load both models and verify outputs"""
    print("="*70)
    print("LOADING MODELS")
    print("="*70)
    
    # Load Audio
    print("\nüéµ Audio model...")
    audio_model = AudioXceptionClassifier(num_classes=2).to(device)
    audio_ckpt = load_checkpoint_auto(audio_model, audio_model_path, is_video_model=False)
    audio_model.eval()
    print(f"‚úÖ Loaded! Epoch: {audio_ckpt.get('epoch', 'N/A')}")
    
    # Load Video - MUST USE SAME ARCHITECTURE AS TRAINING!
    # Training: timm.create_model('xception', pretrained=True) + reset_classifier(2)
    print("\nüé¨ Video model...")
    video_model = VideoXceptionModel(num_classes=2).to(device)
    video_ckpt = load_checkpoint_auto(video_model, video_model_path, is_video_model=True)
    video_model.eval()
    print(f"‚úÖ Loaded! Epoch: {video_ckpt.get('epoch', 'N/A')}")
    
    # Verify outputs
    print("\n" + "="*70)
    print("üîç MODEL OUTPUT VERIFICATION")
    print("="*70)
    
    with torch.no_grad():
        dummy_input = torch.randn(2, 3, 299, 299).to(device)
        
        audio_out = audio_model(dummy_input)
        audio_probs = F.softmax(audio_out, dim=1)
        
        print("\nüéµ AUDIO MODEL:")
        print(f"   Output shape: {audio_out.shape}")
        print(f"   Sample probs [FAKE, REAL]: {audio_probs[0].cpu().numpy()}")
        
        video_out = video_model(dummy_input)
        video_probs = F.softmax(video_out, dim=1)
        
        print("\nüé¨ VIDEO MODEL:")
        print(f"   Output shape: {video_out.shape}")
        print(f"   Sample probs [FAKE, REAL]: {video_probs[0].cpu().numpy()}")
    
    print("\n" + "="*70)
    print("‚úÖ BOTH MODELS READY!")
    print("="*70)
    
    return audio_model, video_model


# ============================================================================
# AUDIO PREPROCESSING
# ============================================================================
def extract_audio_from_video(video_path, output_path="temp_audio.wav", sr=16000):
    """Extract audio from video using ffmpeg"""
    import subprocess
    try:
        cmd = [
            'ffmpeg', '-i', video_path,
            '-vn', '-acodec', 'pcm_s16le',
            '-ar', str(sr), '-ac', '1',
            output_path, '-y'
        ]
        result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, 
                               timeout=60)
        if result.returncode == 0 and os.path.exists(output_path):
            return output_path
        else:
            return None
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
        print(f"‚ö†Ô∏è  Audio extraction error: {e}")
        return None


def preprocess_audio(audio_path, sr=16000, window_size=2.0, overlap=0.5):
    """
    Preprocess audio into mel-spectrograms with overlapping windows
    
    Args:
        audio_path: Path to audio file
        sr: Sample rate
        window_size: Window size in seconds
        overlap: Overlap ratio (0.5 = 50% overlap)
    """
    try:
        y, sr = librosa.load(audio_path, sr=sr)
    except Exception as e:
        print(f"‚ö†Ô∏è  Error loading audio: {e}")
        return []
    win_len = int(window_size * sr)
    hop_len = int(win_len * (1 - overlap))  # 50% overlap = hop = 50% of window
    mel_spectrograms = []

    for start in range(0, len(y), hop_len):
        end = start + win_len
        y_slice = y[start:end]

        # Skip if too short (less than 80% of window)
        if len(y_slice) < int(0.8 * win_len):
            break
        
        # Pad if needed
        if len(y_slice) < win_len:
            y_slice = np.pad(y_slice, (0, win_len - len(y_slice)))

        mel = librosa.feature.melspectrogram(
            y=y_slice, sr=sr,
            n_fft=2048, hop_length=512,
            n_mels=128, fmin=20, fmax=8000
        )
        mel_db = librosa.power_to_db(mel, ref=1.0)
        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-8)

        mel_resized = torch.nn.functional.interpolate(
            torch.from_numpy(mel_db).unsqueeze(0).unsqueeze(0).float(),
            size=(299, 299),
            mode='bilinear',
            align_corners=False
        ).squeeze()

        mel_rgb = mel_resized.repeat(3, 1, 1)
        mel_spectrograms.append(mel_rgb)

    return mel_spectrograms


# ============================================================================
# VIDEO PREPROCESSING
# ============================================================================
def setup_face_detection():
    """Setup face detection models"""
    import urllib.request
    
    # Download Haar Cascade
    if not os.path.exists('haarcascade_frontalface_default.xml'):
        url = 'https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml'
        urllib.request.urlretrieve(url, 'haarcascade_frontalface_default.xml')
    
    haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    
    # Download DNN model
    if not os.path.exists('deploy.prototxt'):
        url = 'https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt'
        urllib.request.urlretrieve(url, 'deploy.prototxt')
    
    if not os.path.exists('res10_300x300_ssd_iter_140000.caffemodel'):
        url = 'https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel'
        urllib.request.urlretrieve(url, 'res10_300x300_ssd_iter_140000.caffemodel')
    
    dnn_net = cv2.dnn.readNetFromCaffe(
        'deploy.prototxt',
        'res10_300x300_ssd_iter_140000.caffemodel'
    )
    
    return haar_cascade, dnn_net


video_transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])


def detect_face_hybrid(frame, haar_cascade, dnn_net, min_size=80):
    """Hybrid face detection (DNN + Haar Cascade)"""
    h, w = frame.shape[:2]

    # Method 1: DNN (more accurate)
    blob = cv2.dnn.blobFromImage(
        cv2.resize(frame, (300, 300)), 1.0,
        (300, 300), (104.0, 177.0, 123.0)
    )
    dnn_net.setInput(blob)
    detections = dnn_net.forward()

    best_face = None
    max_conf = 0

    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > 0.3:
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            (x, y, x2, y2) = box.astype("int")
            x = max(0, x)
            y = max(0, y)
            x2 = min(w, x2)
            y2 = min(h, y2)
            face_w = x2 - x
            face_h = y2 - y

            if face_w >= min_size and face_h >= min_size:
                if confidence > max_conf:
                    best_face = (x, y, face_w, face_h, 'DNN')
                    max_conf = confidence

    # Method 2: Haar Cascade (fallback)
    if best_face is None:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = haar_cascade.detectMultiScale(
            gray, scaleFactor=1.05, minNeighbors=3,
            minSize=(min_size, min_size)
        )
        if len(faces) > 0:
            faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)
            x, y, w, h = faces[0]
            best_face = (x, y, w, h, 'Haar')

    return best_face


# ============================================================================
# MULTIMODAL PROCESSING (FIXED)
# ============================================================================
def process_video_multimodal(video_path, video_model, audio_model, fusion_module,
                             haar_cascade, dnn_net,
                             frame_skip=5, max_frames=500, tau=1.0, 
                             audio_temperature=2.0, audio_overlap=0.5,
                             fusion_strategy='adaptive', verbose=True):
    """
    Main multimodal processing pipeline - FIXED VERSION
    """
    
    if verbose:
        print("\n" + "="*70)
        print("üéØ MULTIMODAL PROCESSING STARTED")
        print("="*70)

    # PHASE 1: AUDIO
    if verbose:
        print("\nüéµ Phase 1: Audio Processing")
    
    audio_path = extract_audio_from_video(video_path)
    audio_available = False
    audio_mels = []
    
    if audio_path and os.path.exists(audio_path):
        if verbose:
            print("‚úÖ Audio extracted")
        # Get audio duration for info
        y, sr = librosa.load(audio_path, sr=16000)
        audio_duration = len(y) / sr
        audio_mels = preprocess_audio(audio_path, overlap=audio_overlap)
        if verbose:
            print(f"‚úÖ Audio duration: {audio_duration:.1f}s")
            print(f"‚úÖ Extracted {len(audio_mels)} audio windows (overlap={audio_overlap*100:.0f}%)")
            if len(audio_mels) < 5:
                print(f"   ‚ö†Ô∏è  Warning: Only {len(audio_mels)} windows - video might be short or audio extraction issue")
        audio_available = len(audio_mels) > 0

    # PHASE 2: VIDEO
    if verbose:
        print("\nüé¨ Phase 2: Video Processing")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("‚ùå Cannot open video!")
        return None

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if verbose:
        print(f"üìπ {total_frames} frames @ {fps}fps")

    results = {
        'video_logits': [],
        'audio_logits': [],
        'video_probs': [],
        'audio_probs': [],
        'fused_probs': [],
        'video_reliability': [],
        'audio_reliability': [],
        'fusion_weights': [],
        'frame_numbers': []
    }

    frame_count = 0
    faces_found = 0

    video_model.eval()
    audio_model.eval()

    with torch.no_grad():
        pbar = tqdm(total=min(total_frames, max_frames), desc="Processing", disable=not verbose)

        while cap.isOpened() and frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % frame_skip == 0:
                face_result = detect_face_hybrid(frame, haar_cascade, dnn_net)

                if face_result is not None:
                    x, y, w, h, method = face_result

                    face_img = frame[y:y+h, x:x+w]
                    face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
                    face_tensor = video_transform(face_pil).unsqueeze(0).to(device)

                    # VIDEO PREDICTION
                    video_logits = video_model(face_tensor)
                    video_probs = F.softmax(video_logits, dim=1)
                    r_video = fusion_module.compute_reliability(video_logits)

                    # AUDIO PREDICTION
                    if audio_available:
                        # Better audio frame mapping: distribute audio windows across video frames
                        # Use modulo to cycle through audio windows if we have more faces than audio windows
                        audio_idx = faces_found % len(audio_mels) if len(audio_mels) > 0 else 0
                        audio_tensor = audio_mels[audio_idx].unsqueeze(0).to(device)
                        audio_logits_raw = audio_model(audio_tensor)
                        
                        # üî• FIX: Temperature scaling to reduce overconfidence
                        # Higher temperature = softer probabilities (less confident)
                        audio_logits = audio_logits_raw / audio_temperature
                        audio_probs = F.softmax(audio_logits, dim=1)
                        r_audio = fusion_module.compute_reliability(audio_logits)
                        
                        # Debug: Show calibration effect (only first frame)
                        if faces_found == 0 and verbose:
                            raw_probs = F.softmax(audio_logits_raw, dim=1)
                            print(f"\nüîß Audio Calibration:")
                            print(f"   Raw probs:     [FAKE={raw_probs[0,0]:.3f}, REAL={raw_probs[0,1]:.3f}]")
                            print(f"   Calibrated:   [FAKE={audio_probs[0,0]:.3f}, REAL={audio_probs[0,1]:.3f}]")
                            print(f"   Temperature:  {audio_temperature}")

                        # ADAPTIVE FUSION
                        # Support different fusion strategies
                        if fusion_strategy == 'equal':
                            # üî¨ EQUAL/BALANCED MODE: 50% video, 50% audio
                            # Tidak ada bias, murni rata-rata kedua modality
                            alpha_v = torch.tensor(0.5).to(device)
                            alpha_a = torch.tensor(0.5).to(device)
                        elif fusion_strategy == 'video_trust':
                            # üî• FIXED: Now video model should be more accurate!
                            # After architecture fix, video should give proper predictions
                            
                            video_fake_prob = video_probs[0, 0].item()
                            video_real_prob = video_probs[0, 1].item()
                            video_conf = max(video_fake_prob, video_real_prob)
                            
                            # Trust video more, but use audio for confirmation
                            if video_conf > 0.6:
                                # Video confident ‚Üí 90% video, 10% audio
                                alpha_v = torch.tensor(0.9).to(device)
                                alpha_a = torch.tensor(0.1).to(device)
                            elif video_fake_prob > video_real_prob:
                                # Video says FAKE ‚Üí trust video for FAKE detection
                                alpha_v = torch.tensor(0.85).to(device)
                                alpha_a = torch.tensor(0.15).to(device)
                            else:
                                # Video says REAL but not confident ‚Üí 70% video, 30% audio
                                alpha_v = torch.tensor(0.7).to(device)
                                alpha_a = torch.tensor(0.3).to(device)
                        else:
                            # Adaptive (default)
                            alpha_v, alpha_a = fusion_module.compute_adaptive_weights(
                                r_video, r_audio, tau=tau
                            )
                            
                            # üî• FIX: Force minimum weight (prevent one modality from dominating)
                            min_weight = 0.2  # Each modality gets at least 20%
                            if alpha_v < min_weight:
                                alpha_v = min_weight
                                alpha_a = 1.0 - min_weight
                            elif alpha_a < min_weight:
                                alpha_a = min_weight
                                alpha_v = 1.0 - min_weight
                        
                        fused_probs = fusion_module.fused_prediction(
                            video_logits, audio_logits, alpha_v, alpha_a
                        )

                        results['audio_logits'].append(audio_logits.cpu().numpy())
                        results['audio_probs'].append(audio_probs.cpu().numpy())
                        results['audio_reliability'].append(r_audio.item())
                        results['fusion_weights'].append((alpha_v.item(), alpha_a.item()))
                    else:
                        fused_probs = video_probs
                        results['fusion_weights'].append((1.0, 0.0))

                    results['video_logits'].append(video_logits.cpu().numpy())
                    results['video_probs'].append(video_probs.cpu().numpy())
                    results['fused_probs'].append(fused_probs.cpu().numpy())
                    results['video_reliability'].append(r_video.item())
                    results['frame_numbers'].append(frame_count)

                    faces_found += 1

                pbar.update(frame_skip)
            frame_count += 1

        pbar.close()

    cap.release()

    if verbose:
        print(f"\n‚úÖ Processed {faces_found} faces")

    return results


# ============================================================================
# ANALYSIS
# ============================================================================
def analyze_results(results, ground_truth_label=None, decision_threshold=0.5,
                    enable_abstention=False, abstention_threshold=0.6):
    """Analyze multimodal results"""
    
    print("\n" + "="*70)
    print("üìä ANALYSIS")
    print("="*70)

    video_probs = np.array(results['video_probs']).squeeze()
    fused_probs = np.array(results['fused_probs']).squeeze()

    video_real = video_probs[:, 1]
    fused_real = fused_probs[:, 1]
    
    fusion_weights = np.array(results['fusion_weights'])

    # Detailed analysis
    video_fake = video_probs[:, 0]
    video_pred = 1 if video_real.mean() > 0.5 else 0
    
    print(f"\nüé¨ VIDEO MODALITY:")
    print(f"   Mean FAKE prob: {video_fake.mean():.3f}")
    print(f"   Mean REAL prob: {video_real.mean():.3f}")
    print(f"   Prediction: {'REAL' if video_pred == 1 else 'FAKE'}")
    print(f"   Weight in fusion: {fusion_weights[:, 0].mean():.3f} ({fusion_weights[:, 0].mean()*100:.1f}%)")
    
    audio_pred = None
    audio_fake = None
    audio_real = None
    if len(results['audio_probs']) > 0:
        audio_probs = np.array(results['audio_probs']).squeeze()
        audio_fake = audio_probs[:, 0]
        audio_real = audio_probs[:, 1]
        audio_pred = 1 if audio_real.mean() > 0.5 else 0
        
        print(f"\nüéµ AUDIO MODALITY:")
        print(f"   Mean FAKE prob: {audio_fake.mean():.3f}")
        print(f"   Mean REAL prob: {audio_real.mean():.3f}")
        print(f"   Prediction: {'REAL' if audio_pred == 1 else 'FAKE'}")
        print(f"   Weight in fusion: {fusion_weights[:, 1].mean():.3f} ({fusion_weights[:, 1].mean()*100:.1f}%)")
        
        # Check if weights are balanced
        avg_video_weight = fusion_weights[:, 0].mean()
        avg_audio_weight = fusion_weights[:, 1].mean()
        
        if avg_video_weight < 0.3 or avg_audio_weight < 0.3:
            print(f"\n‚ö†Ô∏è  WARNING: Fusion weights are imbalanced!")
            print(f"   Video: {avg_video_weight:.3f}, Audio: {avg_audio_weight:.3f}")
            print(f"   Try increasing AUDIO_TEMPERATURE or decreasing tau")
    
    fused_fake = fused_probs[:, 0]
    print(f"\nüîÄ FUSED PREDICTION:")
    print(f"   Mean FAKE prob: {fused_fake.mean():.3f}")
    print(f"   Mean REAL prob: {fused_real.mean():.3f}")

    # üî¨ DECISION STRATEGY - CONFIGURABLE FOR EXPERIMENTS
    # Get fusion strategy from config (passed via decision_threshold hack or global)
    
    if audio_pred is not None:
        video_conf = max(video_fake.mean(), video_real.mean())
        audio_conf = max(audio_fake.mean(), audio_real.mean())
        
        # EQUAL MODE: Murni berdasarkan fused probability, tidak ada bias
        # Ini untuk eksperimen membandingkan dengan video_trust mode
        
        # Check fusion weight to determine strategy mode
        avg_video_weight = fusion_weights[:, 0].mean()
        
        if abs(avg_video_weight - 0.5) < 0.1:  # EQUAL MODE (weights ~50/50)
            # üî¨ EQUAL/BALANCED: Murni berdasarkan fused probability
            predicted_label = 1 if fused_real.mean() > decision_threshold else 0
            print(f"\n   [EQUAL MODE] Using pure fused probability ({fused_real.mean():.3f})")
            print(f"   Threshold: {decision_threshold}, Prediction: {'REAL' if predicted_label == 1 else 'FAKE'}")
        else:  # VIDEO_TRUST or ADAPTIVE MODE
            # Trust video more
            if video_conf > 0.6:
                predicted_label = video_pred
                print(f"\n   [VIDEO_TRUST] Video confident ({video_conf:.3f}) ‚Üí {'REAL' if video_pred == 1 else 'FAKE'}")
            elif video_pred == 0:
                predicted_label = 0
                print(f"\n   [VIDEO_TRUST] Video says FAKE ({video_fake.mean():.3f}) ‚Üí FAKE")
            elif video_pred == 1 and audio_pred == 1:
                predicted_label = 1
                print(f"\n   [VIDEO_TRUST] Both say REAL ‚Üí REAL")
            elif video_pred == 1 and audio_pred == 0 and audio_conf > 0.7:
                predicted_label = 0
                print(f"\n   [VIDEO_TRUST] Video REAL but audio confident FAKE ‚Üí FAKE")
            else:
                predicted_label = video_pred
                print(f"\n   [VIDEO_TRUST] Default: Trust video ‚Üí {'REAL' if video_pred == 1 else 'FAKE'}")
    else:
        predicted_label = video_pred
        print(f"\n   [NO AUDIO] Trust video only")
    
    confidence = fused_real.mean() if predicted_label == 1 else (1 - fused_real.mean())
    
    # ============================================================================
    # üî¨ RESEARCH PROPOSAL Q2: PREDICTION ABSTENTION
    # "What is the best confidence threshold for prediction abstention that 
    # maximizes accuracy on confident predictions while minimizing false 
    # classifications on uncertain samples?"
    # ============================================================================
    abstained = False
    if enable_abstention and confidence < abstention_threshold:
        abstained = True
        verdict = "üü° UNCERTAIN (ABSTAINED)"
        print(f"\n   [ABSTENTION] Confidence ({confidence:.3f}) < threshold ({abstention_threshold})")
        print(f"   Model abstains from making a prediction to avoid false classification")
    else:
        verdict = "[OK] REAL" if predicted_label == 1 else "[!!] FAKE"
    
    # Show individual predictions
    if len(results['audio_probs']) > 0 and audio_pred is not None:
        print(f"\n   INDIVIDUAL PREDICTIONS:")
        print(f"   Video says: {'REAL' if video_pred == 1 else 'FAKE'}")
        print(f"   Audio says: {'REAL' if audio_pred == 1 else 'FAKE'}")
        print(f"   Fused says: {'REAL' if predicted_label == 1 else 'FAKE'}")
        
        if video_pred != audio_pred:
            print(f"   [!] Modalities disagree! Fusion will decide.")

    print("\n" + "="*70)
    print("VERDICT")
    print("="*70)
    print(f"\n  {verdict}")
    print(f"  Confidence: {confidence:.3f}")
    
    if abstained:
        print(f"  [ABSTENTION ACTIVE] Model is uncertain - requires human review")

    if ground_truth_label is not None:
        gt_str = "REAL" if ground_truth_label == 1 else "FAKE"
        is_correct = (predicted_label == ground_truth_label)
        print(f"\n  Ground Truth: {gt_str}")
        print(f"  {'‚úÖ CORRECT' if is_correct else '‚ùå INCORRECT'}")

    print("="*70)

    return {
        'predicted_label': predicted_label,
        'confidence': confidence,
        'fused_real_prob': fused_real.mean(),
        'abstained': abstained,  # üî¨ Q2: Prediction Abstention
        'video_pred': video_pred,
        'audio_pred': audio_pred,
        'video_probs': {'fake': video_fake.mean(), 'real': video_real.mean()},
        'audio_probs': {'fake': audio_fake.mean() if audio_fake is not None else None, 
                        'real': audio_real.mean() if audio_real is not None else None},
        'fusion_weights': {'video': fusion_weights[:, 0].mean(), 
                          'audio': fusion_weights[:, 1].mean()}
    }


# ============================================================================
# MAIN USAGE EXAMPLE
# ============================================================================
if __name__ == "__main__":
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     MULTIMODAL DEEPFAKE DETECTION - FIXED VERSION         ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # CONFIGURE THESE PATHS
    AUDIO_MODEL_PATH = "best_audio_xception.pth"
    VIDEO_MODEL_PATH = "epoch_007.pt"
    
    # ============================================================================
    # üîß CONFIGURATION - UBAH INI UNTUK OPTIMAL PERFORMANCE
    # ============================================================================
    
    # ============================================================================
    # üèÜ OPTIMAL CONFIGURATION (Fixed for Audio Model Bias)
    # ============================================================================
    # ISSUE: Audio model has 785 false negatives (FAKE ‚Üí REAL) in test set
    # SOLUTION: Trust video more + higher temperature + lower threshold
    
    # Audio temperature scaling (to reduce overconfidence)
    # üî¨ EXPERIMENT: Set lebih rendah untuk mode EQUAL agar lebih fair
    # - 1.0 = No scaling (raw output)
    # - 2.0-5.0 = Light scaling
    # - 10.0 = Heavy scaling (reduce overconfidence drastis)
    AUDIO_TEMPERATURE = 2.0  # Turun dari 10.0 untuk eksperimen EQUAL mode
    
    # Fusion temperature (tau) - controls sensitivity to reliability differences
    # LOWERED: Untuk lebih balanced, tidak terlalu sensitive ke audio confidence
    FUSION_TAU = 0.2  # Turun dari 0.3
    
    # Decision threshold - probability threshold untuk classify REAL vs FAKE
    # üî¨ EXPERIMENT: 0.5 = balanced threshold untuk mode EQUAL
    DECISION_THRESHOLD = 0.5  # Standard 50/50 threshold
    
    # Fusion strategy: 'adaptive', 'equal', 'video_trust'
    # OPTIONS:
    #   - 'equal': 50% video, 50% audio (BALANCED - untuk eksperimen)
    #   - 'video_trust': Trust video lebih (85% video, 15% audio)
    #   - 'adaptive': Dynamic weights berdasarkan confidence
    # üî¨ EXPERIMENT MODE: Coba 'equal' untuk lihat perbandingan
    FUSION_STRATEGY = 'equal'  # BALANCED 50/50 untuk eksperimen
    
    # Audio window overlap (0.5 = 50% overlap = more windows from same audio)
    AUDIO_OVERLAP = 0.5  # 50% overlap means 2x more windows
    
    # ============================================================================
    # üî¨ RESEARCH PROPOSAL FEATURES (Q2: Prediction Abstention)
    # ============================================================================
    # Abstention: Jika confidence < threshold, model ABSTAIN (tidak predict)
    # Ini untuk "minimize false classifications on uncertain samples"
    ENABLE_ABSTENTION = True  # Set False untuk disable
    ABSTENTION_THRESHOLD = 0.6  # Confidence minimum untuk membuat prediksi
    # Jika confidence < 0.6, output = "UNCERTAIN" bukan REAL/FAKE
    
    # File browser for video selection
    try:
        from tkinter import filedialog
        import tkinter as tk
        
        print("\nüìÅ Opening file browser to select video...")
        root = tk.Tk()
        root.withdraw()  # Hide the main window
        
        video_path = filedialog.askopenfilename(
            title="Select Video File",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.flv *.wmv"),
                ("All files", "*.*")
            ]
        )
        
        if not video_path:
            print("‚ùå No video selected. Exiting.")
            exit(0)
        
        print(f"‚úÖ Selected: {video_path}")
        
        # Ask for ground truth label
        print("\n‚ùì What is the ground truth label for this video?")
        print("   Enter '0' for FAKE, '1' for REAL, or press Enter to skip")
        gt_input = input("Ground truth label (0/1 or Enter): ").strip()
        
        if gt_input in ['0', '1']:
            ground_truth = int(gt_input)
            gt_str = "FAKE" if ground_truth == 0 else "REAL"
            print(f"‚úÖ Ground truth set to: {gt_str}")
        else:
            ground_truth = None
            print("‚ö†Ô∏è  No ground truth provided (will skip accuracy check)")
        
    except ImportError:
        print("‚ö†Ô∏è  tkinter not available. Please enter video path manually:")
        video_path = input("Video path: ").strip().strip('"').strip("'")
        
        if not video_path or not os.path.exists(video_path):
            print("‚ùå Invalid path. Exiting.")
            exit(1)
        
        print("\n‚ùì What is the ground truth label? (0=FAKE, 1=REAL, Enter=skip)")
        gt_input = input("Ground truth label: ").strip()
        ground_truth = int(gt_input) if gt_input in ['0', '1'] else None
    
    # Load models
    print("\n" + "="*70)
    print("ü§ñ LOADING MODELS")
    print("="*70)
    
    if not os.path.exists(AUDIO_MODEL_PATH):
        print(f"‚ùå Audio model not found: {AUDIO_MODEL_PATH}")
        print("   Please check the path!")
        exit(1)
    
    if not os.path.exists(VIDEO_MODEL_PATH):
        print(f"‚ùå Video model not found: {VIDEO_MODEL_PATH}")
        print("   Please check the path!")
        exit(1)
    
    audio_model, video_model = load_models(AUDIO_MODEL_PATH, VIDEO_MODEL_PATH)
    fusion_module = AdaptiveFusionModule()
    haar_cascade, dnn_net = setup_face_detection()
    
    # Process video
    print("\n" + "="*70)
    print("üé¨ PROCESSING VIDEO")
    print("="*70)
    
    results = process_video_multimodal(
        video_path=video_path,
        video_model=video_model,
        audio_model=audio_model,
        fusion_module=fusion_module,
        haar_cascade=haar_cascade,
        dnn_net=dnn_net,
        tau=FUSION_TAU,  # Lower tau = more balanced weights
        audio_temperature=AUDIO_TEMPERATURE,  # üî• FIX: Reduce audio overconfidence
        audio_overlap=AUDIO_OVERLAP,  # 50% overlap = more audio windows
        fusion_strategy=FUSION_STRATEGY,  # Fusion strategy
        verbose=True
    )

    if results and len(results['video_probs']) > 0:
        # Analyze results
        print("\n" + "="*70)
        print("üìä ANALYZING RESULTS")
        print("="*70)
        
        analysis = analyze_results(results, ground_truth_label=ground_truth, 
                                   decision_threshold=DECISION_THRESHOLD,
                                   enable_abstention=ENABLE_ABSTENTION,
                                   abstention_threshold=ABSTENTION_THRESHOLD)
        
        print("\n" + "="*70)
        print("[OK] DETECTION COMPLETE!")
        print("="*70)
        print(f"\n  Video: {os.path.basename(video_path)}")
        
        if analysis.get('abstained', False):
            print(f"  Prediction: UNCERTAIN (ABSTAINED)")
            print(f"  Reason: Confidence ({analysis['confidence']:.3f}) < {ABSTENTION_THRESHOLD}")
            print(f"  [!] Requires human review")
        else:
            print(f"  Prediction: {'REAL' if analysis['predicted_label'] == 1 else 'FAKE'}")
            print(f"  Confidence: {analysis['confidence']:.3f}")
        
        print(f"\n  Fusion Weights:")
        print(f"    Video: {analysis['fusion_weights']['video']*100:.1f}%")
        print(f"    Audio: {analysis['fusion_weights']['audio']*100:.1f}%")
        
        if ground_truth is not None and not analysis.get('abstained', False):
            is_correct = (analysis['predicted_label'] == ground_truth)
            print(f"\n  Ground Truth: {'REAL' if ground_truth == 1 else 'FAKE'}")
            print(f"  Result: {'[OK] CORRECT' if is_correct else '[X] WRONG'}")
    else:
        print("\n‚ùå No faces detected in video!")
        print("   Try a different video or check face detection settings.")

