{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. DOWNLOAD DATASET FROM KAGGLE\n",
        "# ============================================================\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "dataset_path = kagglehub.dataset_download(\"mbulsss/fakeavceleb\")\n",
        "print(\"Path to dataset files:\", dataset_path)\n",
        "\n",
        "\n",
        "BASE_PATH = os.path.join(dataset_path, \"FakeAVCeleb_v1.2\")\n",
        "meta_path = os.path.join(BASE_PATH, \"meta_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ_lPfdEYDCo",
        "outputId": "45a3fce4-bc23-465d-d2f4-f66b9e141cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mbulsss/fakeavceleb?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.97G/5.97G [02:34<00:00, 41.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mbulsss/fakeavceleb/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0) MOUNT DRIVE & SET PROJECT DIR\n",
        "# ============================================================\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/FinalProjectAI\"\n",
        "SPLIT_DIR   = os.path.join(PROJECT_DIR, \"splits_identity\")\n",
        "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Project dir :\", PROJECT_DIR)\n",
        "print(\"Split dir   :\", SPLIT_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# 1) LOAD METADATA ASLI DARI DATASET\n",
        "# ============================================================\n",
        "DATA_ROOT = \"/root/.cache/kagglehub/datasets/mbulsss/fakeavceleb/versions/1\"\n",
        "META_PATH = os.path.join(DATA_ROOT, \"FakeAVCeleb_v1.2\", \"meta_data.csv\")  # sesuaikan jika namanya beda\n",
        "\n",
        "print(\"Metadata path:\", META_PATH)\n",
        "all_videos = pd.read_csv(META_PATH)\n",
        "print(\"Metadata shape:\", all_videos.shape)\n",
        "print(all_videos.head())\n",
        "print(\"Columns:\", all_videos.columns.tolist())\n",
        "\n",
        "# Harus minimal punya kolom:\n",
        "# ['source','target1','target2','method','category','type','race','gender','path', ...]\n",
        "# Di beberapa versi, ada kolom 'rel_dir' juga; kalau belum ada, kita bikin.\n",
        "\n",
        "# ============================================================\n",
        "# 2) NORMALISASI KOLOM & TAMBAH rel_dir\n",
        "# ============================================================\n",
        "all_videos = all_videos.rename(columns={\n",
        "    \"source\": \"identity_id\"   # id00076, dll\n",
        "})\n",
        "\n",
        "# Kalau metadata kamu sudah punya 'rel_dir', biarkan saja.\n",
        "if \"rel_dir\" not in all_videos.columns:\n",
        "    # Bangun rel_dir dari struktur folder standar FakeAVCeleb_v1.2\n",
        "    # misal: FakeAVCeleb_v1.2/RealVideo-RealAudio/African/men/id00076\n",
        "    def build_rel_dir(row):\n",
        "        return os.path.join(\n",
        "            \"FakeAVCeleb_v1.2\",\n",
        "            row[\"type\"],\n",
        "            row[\"race\"],\n",
        "            row[\"gender\"],\n",
        "            row[\"identity_id\"]\n",
        "        )\n",
        "    all_videos[\"rel_dir\"] = all_videos.apply(build_rel_dir, axis=1)\n",
        "\n",
        "# Pastikan kolom path ada (nama file video, mis: 00109.mp4)\n",
        "if \"path\" not in all_videos.columns:\n",
        "    raise ValueError(\"Kolom 'path' (nama file video) tidak ditemukan di metadata.\")\n",
        "\n",
        "print(\"\\nSample rows after normalize:\")\n",
        "print(all_videos[[\"identity_id\", \"type\", \"race\", \"gender\", \"path\", \"rel_dir\"]].head())\n",
        "\n",
        "# ============================================================\n",
        "# 3) LABEL BINER: 1 = REAL, 0 = FAKE\n",
        "# ============================================================\n",
        "def map_binary_label(t):\n",
        "    if t == \"RealVideo-RealAudio\":\n",
        "        return 1  # REAL\n",
        "    else:\n",
        "        return 0  # FAKE\n",
        "\n",
        "all_videos[\"binary_label\"] = all_videos[\"type\"].apply(map_binary_label)\n",
        "\n",
        "print(\"\\nBinary label distribution (1=REAL, 0=FAKE):\")\n",
        "print(all_videos[\"binary_label\"].value_counts())\n",
        "\n",
        "# ============================================================\n",
        "# 4) IDENTITY-BASED SPLIT 70/15/15\n",
        "# ============================================================\n",
        "identity_info = all_videos[[\"identity_id\", \"binary_label\"]].copy()\n",
        "\n",
        "# Satu label per identity untuk stratify (mayoritas)\n",
        "identity_info = (\n",
        "    identity_info\n",
        "    .groupby(\"identity_id\")[\"binary_label\"]\n",
        "    .agg(lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0])\n",
        "    .reset_index()\n",
        "    .rename(columns={\"binary_label\": \"identity_label\"})\n",
        ")\n",
        "\n",
        "# Buang identity kosong kalau ada\n",
        "identity_info = identity_info[identity_info[\"identity_id\"] != \"\"].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nUnique identities:\", len(identity_info))\n",
        "print(identity_info.head())\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# 4.1) train+val vs test (85/15)\n",
        "train_val_ids, test_ids = train_test_split(\n",
        "    identity_info[\"identity_id\"].values,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=identity_info[\"identity_label\"].values\n",
        ")\n",
        "\n",
        "# 4.2) train vs val dari train+val (70/15) → val fraction = 15/85 ≈ 0.176\n",
        "train_val_info = identity_info[identity_info[\"identity_id\"].isin(train_val_ids)].copy()\n",
        "\n",
        "train_ids, val_ids = train_test_split(\n",
        "    train_val_info[\"identity_id\"].values,\n",
        "    test_size=0.176,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=train_val_info[\"identity_label\"].values\n",
        ")\n",
        "\n",
        "print(\"\\nIdentity counts per split:\")\n",
        "print(\"Train identities :\", len(train_ids))\n",
        "print(\"Val identities   :\", len(val_ids))\n",
        "print(\"Test identities  :\", len(test_ids))\n",
        "\n",
        "# ============================================================\n",
        "# 5) TURUNKAN KE LEVEL VIDEO DAN SIMPAN CSV\n",
        "# ============================================================\n",
        "train_df = all_videos[all_videos[\"identity_id\"].isin(train_ids)].reset_index(drop=True)\n",
        "val_df   = all_videos[all_videos[\"identity_id\"].isin(val_ids)].reset_index(drop=True)\n",
        "test_df  = all_videos[all_videos[\"identity_id\"].isin(test_ids)].reset_index(drop=True)\n",
        "\n",
        "def show_dist(name, df):\n",
        "    total = len(df)\n",
        "    counts = df[\"binary_label\"].value_counts().to_dict()\n",
        "    print(f\"{name}: total={total}, distribusi={counts}\")\n",
        "\n",
        "print(\"\\nBinary label distribution per split (1=REAL, 0=FAKE):\")\n",
        "show_dist(\"TRAIN\", train_df)\n",
        "show_dist(\"VAL\",   val_df)\n",
        "show_dist(\"TEST\",  test_df)\n",
        "\n",
        "# Simpan ke Drive\n",
        "train_path = os.path.join(SPLIT_DIR, \"train_videos_identity.csv\")\n",
        "val_path   = os.path.join(SPLIT_DIR, \"val_videos_identity.csv\")\n",
        "test_path  = os.path.join(SPLIT_DIR, \"test_videos_identity.csv\")\n",
        "\n",
        "train_df.to_csv(train_path, index=False)\n",
        "val_df.to_csv(val_path, index=False)\n",
        "test_df.to_csv(test_path, index=False)\n",
        "\n",
        "print(\"\\nSaved to Google Drive:\")\n",
        "print(\"TRAIN:\", train_path)\n",
        "print(\"VAL  :\", val_path)\n",
        "print(\"TEST :\", test_path)\n",
        "\n",
        "# Cek tidak ada identity overlap\n",
        "train_ids_set = set(train_ids)\n",
        "val_ids_set   = set(val_ids)\n",
        "test_ids_set  = set(test_ids)\n",
        "\n",
        "print(\"\\nLeakage check (should all be 0):\")\n",
        "print(\"Train ∩ Val :\", len(train_ids_set & val_ids_set))\n",
        "print(\"Train ∩ Test:\", len(train_ids_set & test_ids_set))\n",
        "print(\"Val ∩ Test  :\", len(val_ids_set & test_ids_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkFKMkyqkXi7",
        "outputId": "97932ece-486e-4325-fddb-fa84501ce627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project dir : /content/drive/MyDrive/FinalProjectAI\n",
            "Split dir   : /content/drive/MyDrive/FinalProjectAI/splits_identity\n",
            "Metadata path: /root/.cache/kagglehub/datasets/mbulsss/fakeavceleb/versions/1/FakeAVCeleb_v1.2/meta_data.csv\n",
            "Metadata shape: (21566, 10)\n",
            "    source target1 target2 method category                 type     race  \\\n",
            "0  id00076       -       -   real        A  RealVideo-RealAudio  African   \n",
            "1  id00166       -       -   real        A  RealVideo-RealAudio  African   \n",
            "2  id00173       -       -   real        A  RealVideo-RealAudio  African   \n",
            "3  id00366       -       -   real        A  RealVideo-RealAudio  African   \n",
            "4  id00391       -       -   real        A  RealVideo-RealAudio  African   \n",
            "\n",
            "  gender       path                                         Unnamed: 9  \n",
            "0    men  00109.mp4  FakeAVCeleb/RealVideo-RealAudio/African/men/id...  \n",
            "1    men  00010.mp4  FakeAVCeleb/RealVideo-RealAudio/African/men/id...  \n",
            "2    men  00118.mp4  FakeAVCeleb/RealVideo-RealAudio/African/men/id...  \n",
            "3    men  00118.mp4  FakeAVCeleb/RealVideo-RealAudio/African/men/id...  \n",
            "4    men  00052.mp4  FakeAVCeleb/RealVideo-RealAudio/African/men/id...  \n",
            "Columns: ['source', 'target1', 'target2', 'method', 'category', 'type', 'race', 'gender', 'path', 'Unnamed: 9']\n",
            "\n",
            "Sample rows after normalize:\n",
            "  identity_id                 type     race gender       path  \\\n",
            "0     id00076  RealVideo-RealAudio  African    men  00109.mp4   \n",
            "1     id00166  RealVideo-RealAudio  African    men  00010.mp4   \n",
            "2     id00173  RealVideo-RealAudio  African    men  00118.mp4   \n",
            "3     id00366  RealVideo-RealAudio  African    men  00118.mp4   \n",
            "4     id00391  RealVideo-RealAudio  African    men  00052.mp4   \n",
            "\n",
            "                                             rel_dir  \n",
            "0  FakeAVCeleb_v1.2/RealVideo-RealAudio/African/m...  \n",
            "1  FakeAVCeleb_v1.2/RealVideo-RealAudio/African/m...  \n",
            "2  FakeAVCeleb_v1.2/RealVideo-RealAudio/African/m...  \n",
            "3  FakeAVCeleb_v1.2/RealVideo-RealAudio/African/m...  \n",
            "4  FakeAVCeleb_v1.2/RealVideo-RealAudio/African/m...  \n",
            "\n",
            "Binary label distribution (1=REAL, 0=FAKE):\n",
            "binary_label\n",
            "0    21066\n",
            "1      500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Unique identities: 500\n",
            "  identity_id  identity_label\n",
            "0     id00018               0\n",
            "1     id00020               0\n",
            "2     id00021               0\n",
            "3     id00025               0\n",
            "4     id00029               0\n",
            "\n",
            "Identity counts per split:\n",
            "Train identities : 350\n",
            "Val identities   : 75\n",
            "Test identities  : 75\n",
            "\n",
            "Binary label distribution per split (1=REAL, 0=FAKE):\n",
            "TRAIN: total=15106, distribusi={0: 14756, 1: 350}\n",
            "VAL: total=3187, distribusi={0: 3112, 1: 75}\n",
            "TEST: total=3273, distribusi={0: 3198, 1: 75}\n",
            "\n",
            "Saved to Google Drive:\n",
            "TRAIN: /content/drive/MyDrive/FinalProjectAI/splits_identity/train_videos_identity.csv\n",
            "VAL  : /content/drive/MyDrive/FinalProjectAI/splits_identity/val_videos_identity.csv\n",
            "TEST : /content/drive/MyDrive/FinalProjectAI/splits_identity/test_videos_identity.csv\n",
            "\n",
            "Leakage check (should all be 0):\n",
            "Train ∩ Val : 0\n",
            "Train ∩ Test: 0\n",
            "Val ∩ Test  : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 0) MOUNT DRIVE & SET PATH\n",
        "# ============================================================\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/FinalProjectAI\"\n",
        "SPLIT_DIR   = os.path.join(PROJECT_DIR, \"splits_identity\")\n",
        "FRAMES_DIR  = os.path.join(PROJECT_DIR, \"frames_identity\")\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
        "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Project dir :\", PROJECT_DIR)\n",
        "print(\"Split dir   :\", SPLIT_DIR)\n",
        "print(\"Frames dir  :\", FRAMES_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# 1) ROOT DATASET (SUDAH ADA DARI kagglehub)\n",
        "# ============================================================\n",
        "BASE_ROOT = \"/root/.cache/kagglehub/datasets/mbulsss/fakeavceleb/versions/1\"\n",
        "print(\"Dataset root:\", BASE_ROOT)\n",
        "\n",
        "# ============================================================\n",
        "# 2) LOAD CSV SPLIT DARI DRIVE\n",
        "# ============================================================\n",
        "train_videos = pd.read_csv(os.path.join(SPLIT_DIR, \"train_videos_identity.csv\"))\n",
        "val_videos   = pd.read_csv(os.path.join(SPLIT_DIR, \"val_videos_identity.csv\"))\n",
        "test_videos  = pd.read_csv(os.path.join(SPLIT_DIR, \"test_videos_identity.csv\"))\n",
        "\n",
        "print(\"Train videos:\", train_videos.shape)\n",
        "print(\"Val videos  :\", val_videos.shape)\n",
        "print(\"Test videos :\", test_videos.shape)\n",
        "print(\"Columns    :\", train_videos.columns.tolist())\n",
        "\n",
        "# Harus minimal ada:\n",
        "# ['identity_id','target1','target2','method','category',\n",
        "#  'type','race','gender','path','rel_dir','binary_label']\n",
        "\n",
        "# ============================================================\n",
        "# 3) FUNGSI BANGUN PATH VIDEO DARI rel_dir + path\n",
        "# ============================================================\n",
        "def get_video_path(row):\n",
        "    rel_dir = str(row[\"rel_dir\"])   # contoh: FakeAVCeleb_v1.2/RealVideo-RealAudio/African/men/id00076\n",
        "    fname   = str(row[\"path\"])      # contoh: 00109.mp4\n",
        "    return os.path.join(BASE_ROOT, rel_dir, fname)\n",
        "\n",
        "# ============================================================\n",
        "# 4) FUNGSI EKSTRAK FRAME\n",
        "# ============================================================\n",
        "def extract_frames_from_video(video_path, out_dir, video_id,\n",
        "                              max_frames=10, frame_step=5):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Failed to open:\", video_path)\n",
        "        return []\n",
        "\n",
        "    saved_paths = []\n",
        "    frame_idx = 0\n",
        "    saved = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Ambil setiap frame_step frame\n",
        "        if frame_idx % frame_step == 0:\n",
        "            out_name = f\"{video_id}_f{frame_idx:05d}.jpg\"\n",
        "            out_path = os.path.join(out_dir, out_name)\n",
        "            cv2.imwrite(out_path, frame)\n",
        "            saved_paths.append(out_path)\n",
        "            saved += 1\n",
        "            if saved >= max_frames:\n",
        "                break\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    return saved_paths\n",
        "\n",
        "# ============================================================\n",
        "# 5) LOOP SPLIT → FRAME-LEVEL CSV\n",
        "# ============================================================\n",
        "def process_split(df, split_name):\n",
        "    records = []\n",
        "    split_dir = os.path.join(FRAMES_DIR, split_name)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Extracting {split_name}\"):\n",
        "        video_name   = str(row[\"path\"])\n",
        "        binary_label = int(row[\"binary_label\"])   # 1=REAL, 0=FAKE\n",
        "        identity_id  = str(row[\"identity_id\"])\n",
        "        video_id     = os.path.splitext(video_name)[0]\n",
        "\n",
        "        video_path = get_video_path(row)\n",
        "        if not os.path.exists(video_path):\n",
        "            print(\"Missing video:\", video_path)\n",
        "            continue\n",
        "\n",
        "        # Simpan ke folder per label: frames_identity/train/0, frames_identity/train/1, dst.\n",
        "        label_dir = os.path.join(split_dir, str(binary_label))\n",
        "\n",
        "        frame_paths = extract_frames_from_video(\n",
        "            video_path=video_path,\n",
        "            out_dir=label_dir,\n",
        "            video_id=video_id,\n",
        "            max_frames=10,   # ganti kalau mau lebih/kurang\n",
        "            frame_step=5     # ganti kalau mau lebih rapat/jarang\n",
        "        )\n",
        "\n",
        "        for fp in frame_paths:\n",
        "            records.append({\n",
        "                \"frame_path\": fp,\n",
        "                \"video_id\": video_id,\n",
        "                \"identity_id\": identity_id,\n",
        "                \"binary_label\": binary_label,\n",
        "                \"split\": split_name\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "train_frames = process_split(train_videos, \"train\")\n",
        "val_frames   = process_split(val_videos, \"val\")\n",
        "test_frames  = process_split(test_videos, \"test\")\n",
        "\n",
        "print(\"Train frames:\", train_frames.shape)\n",
        "print(\"Val frames  :\", val_frames.shape)\n",
        "print(\"Test frames :\", test_frames.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 6) SIMPAN CSV FRAME-LEVEL KE DRIVE\n",
        "# ============================================================\n",
        "train_frames.to_csv(os.path.join(FRAMES_DIR, \"train_frames.csv\"), index=False)\n",
        "val_frames.to_csv(os.path.join(FRAMES_DIR, \"val_frames.csv\"), index=False)\n",
        "test_frames.to_csv(os.path.join(FRAMES_DIR, \"test_frames.csv\"), index=False)\n",
        "\n",
        "print(\"Done. Frame CSVs saved in:\", FRAMES_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR6nDaTrlaOu",
        "outputId": "6793f57a-5933-4017-b616-f8e20dd4d72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project dir : /content/drive/MyDrive/FinalProjectAI\n",
            "Split dir   : /content/drive/MyDrive/FinalProjectAI/splits_identity\n",
            "Frames dir  : /content/drive/MyDrive/FinalProjectAI/frames_identity\n",
            "Dataset root: /root/.cache/kagglehub/datasets/mbulsss/fakeavceleb/versions/1\n",
            "Train videos: (15106, 12)\n",
            "Val videos  : (3187, 12)\n",
            "Test videos : (3273, 12)\n",
            "Columns    : ['identity_id', 'target1', 'target2', 'method', 'category', 'type', 'race', 'gender', 'path', 'Unnamed: 9', 'rel_dir', 'binary_label']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting train: 100%|██████████| 15106/15106 [16:53<00:00, 14.91it/s]\n",
            "Extracting val: 100%|██████████| 3187/3187 [04:24<00:00, 12.06it/s]\n",
            "Extracting test: 100%|██████████| 3273/3273 [04:48<00:00, 11.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train frames: (151014, 5)\n",
            "Val frames  : (31870, 5)\n",
            "Test frames : (32722, 5)\n",
            "Done. Frame CSVs saved in: /content/drive/MyDrive/FinalProjectAI/frames_identity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **========================================**"
      ],
      "metadata": {
        "id": "lxMK3rGWATAF"
      }
    }
  ]
}